---
title: Employing Machine-Learning Approaches in Predicting Incomes of Recent College Graduates
author: "Will Doyle, Benjamin Skinner, Olivia Morales"
subtitle: Proposal for ASHE 2022
output: pdf_document
bibliography: csc_research.bib
csl: apa.csl
---

```{r setup, echo=F, include=F, message=F, warning=F, error=F}

library(knitr)
library(tidyverse)
library(haven)
library(bibtex)
library(ggridges)
library(scales)
library(here)
library(janitor)
library(rmarkdown)

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      include = TRUE,
                      message = FALSE,
                      fig.width = 10,
                      fig.height = 10)

```

**BS: we need some more citations about labor market returns to
college in general and if an article exists, to specific majors. That
would help us build our case for the importance of this work beyond
the technical exercise.**

```{r}
cb_df <- read_rds(here("data","cleaned","cb_df.Rds"))
```

## Abstract  

This project uses a principled machine-learning approach to predict
recent college graduates' earnings using data from the College
Scorecard. Early results support the predictive capabilities of
institutional characteristics like school classification and overall
debt repayment rates on recent graduate earnings.

\newpage

## Objective/Background

The creation and publication of the College Scorecard by the
U.S. Department of Education derived from President Obama's
frustration with lack of institutional transparency as to rising costs
and ultimate goal to increase college affordability and grow the
U.S. middle class [@obama_2013]. The College Scorecard presented an
opportunity for families to identify the institutions that provided
the best outcomes for their students with the least amount of
financial burden. Made publicly available in 2015, the data in the
College Scorecard (while rich and vast in information), did not
generally produce the kind of impact the Obama administration
envisioned. In @hurwitz_student_2018, results indicated
decision-making changes in generally well-resourced high school
students after the publication of the Scorecard, directing their SAT
scores to schools that, on average, had higher median earnings for
graduates; the two other hallmarks of the Scorecard (graduation rates
and average costs), produced virtually no change in SAT score-sending
behaviors.

Accompanied with the less than promising results from
@huntington-klein_search_2017 on the impact of the College Scorecard
release on subsequent college Google search activity, the Scorecard
put forward a wealth of data that were simultaneously underutilized by
the target demographic and generally underemployed by higher education
researchers. While literature does exist engaging the earnings data
and available on the Scorecard in particular institutional & program
contexts [@boland_effect_2021; @elu_earnings_2019; @mabel_value_2020;
@seaman_assessing_2017], the Scorecard is largely missing from
literature and general higher education discourse on college
affordability and broadening college access.

Machine learning, as an academic field of study, shares much in common
with the vastly growing discipline marrying both statistical methods
and computer science. While commonly associated with convoluted
computational statistics and computer programming methods, it has
crept into the education (particularly higher education) field to
increase model accuracy and potential estimates in quantitative higher
education studies. In particular, the last 6-7 years have seen an
uptick in higher education research projects utilizing machine
learning methods [@aulck2017predicting; @savvas_etal_2021;
@Zeineddine_2021]. With this increase in prominence, how does this
project stand out?

This project fills a dire gap in higher education literature by not
only utilizing the myriad institutional data points available on the
Scorecard, but marrying these data with novel machine learning
techniques that improve the predictive capacity of common
institutional characteristics.

**BS: I think our primary contribution is our principled machine
learning / tidy models approach. Other people have predicted earnings
using these data, but not in the way we've done so. In other words,
this is sort of methods/process paper. That means we should probably
refocus the introduction a bit to hit that harder. Right now it feels
more like: College Scorecard as a thing that was weak --> machine
learning to predict earnings. I would do more like: We want to know
program earnings --> have College Scorecard, but weak / missing data
--> common econometric approaches may not cut it --> principled
machine learning approach to help us get the earnings estimates. The
good news is that I think a lot of this text can be reused, just in a
different order / with different emphasis.**

## Methodology

Our methodology is defined by machine-learning approaches to data
analysis, characterized by the use of a model workflow, feature
engineering for model use and elastic net/random forest regression
models to appropriately fit our data and identify potential income
predictors. (**CITE HASTIE BOOK; TIDYMODELS**)

More specifically, we first read in the College Scorecard data (the
field of study and all data elements files, specifically) and
performed necessary preprocessing work to 1) drop data that were
privacy suppressed/missing, 2) recode categorical data to workable
dummy-coded variables and 3) drop zero variance/highly correlated
predictors.

Next, we performed kfold cross validation (20 folds) on the training
set data to set the foundation for model selection/evaluation. Two
regression-based methods (elastic net and random forest) were then
identified to build subsequent models, add models to built workflow
and fit the models to resampled data. Model tuning was then performed
for both models to ensure maximum predictive capacity. Elastic net
regularization is an improved version of the LASSO method that
combines penalties to remove non-predictive coefficients. Random
forest regression produce variable cases and forces a vote on the most
likely outcome for the covariate in question.

These methods resulted in predictive estimates for both the elastic
net and random forest regression models critical to our ultimate
analyses/findings. These predictors are variables identified in the
Scorecard dataset that are highly predictive indicators of our
dependent variable: median earnings from graduates of the program
after 1 year.

**BS: It's always chicken/egg to talk about methods or data
first. Rather than say what we did as a past tense thing, I might use
the simple present and just _slightly_ more abstract. Lead more with
elastic need and regression forests, noting that they are good for
principled feature selection (EN: how do we pick the best predictors
out of this huge list?) and non-linear relationships in predictors
(RF). I like how you end up with the key outcome of interest.**

## Data 

Data for this project originate from two specific sources: the College
Scorecard and the most recent American Community Survey 5-year
estimates (2016-2020), selecting 2019 data to align with the recent
2019-20 school year data featured in the Scorecard. The Scorecard
provided us with our dependent variable data (median earnings for
college graduates 1 year after graduation), and accompanied with the
ACS county-level data, contributes the numerous possible predictors
for our models.

More specifically, the ACS county data feature FIPS codes to uniquely
identify each county, calculations for: 1) the percentage of county
bachelor's degree holders, 2) the percentage of homeowners in each
county and 3) the percentage of individuals identified in the county
labor force and the median household income for each county (for most
recent 12 months, using 2019-adjusted dollars). The College Scorecard
data present 2,000+ variables regarding institutional characteristics
and program-level data for 6,700 accredited institutions in the U.S.,
including type of institution, degrees awarded, number of loan
borrowers and the like. FIPS codes are also featured in this data set,
allowing for appropriate matching of institutions to their location in
each county first identified by the ACS data. Much of the Scorecard
data based in more specific, individual student information were
suppressed for privacy reasons.

**BS: Yes, make this sound huge. That's why we use a principled
approach. If we mention the privacy suppressed stuff, we need to have
a solution rather than just saying ¯\_(ツ)_/¯. I think we can lean
harder on: we use all these extra variables from ACS and the Scorecard
precisely so we can try to recover information lost in that privacy
suppression.**

## Preliminary Findings

In our first 3 figures, we delineate the median first year earnings of
different degree holders (Bachelors, Associates and
Certificates/Diplomas). In looking at this figures descriptively, we
find (**BS: rather than seems, we'll just own it: we find**) an
increase in earnings potential for Bachelors degree holders as
compared to Associates/Certificate degree holders in similar fields of
study. However, this conclusion necessitates further analyses to
determine the predictive capabilities of things other than field of
study (institutional characteristics, student traits, etc.)  in
determining the incomes of recent college graduates. (**BS: it makes
sense that BA/BS make more than AA/AS/Cert in the same field**).

Both the elastic net and random forest regression models produced
estimates to inform the predictive capabilities of certain
program/institutional characteristics. Figure 4 demonstrates those
estimates from elastic net, identifying typically assumed positive
predictors of income like type of school, type of degree/credential;
however, this model also illuminated particularly unexpected
predictors like outstanding Federal loan balance and median debt for
graduated students.

In our random forest regression model (Figure 5), we see a similar
emphasis on the importance of type of degree credential (specifically
Certificates/Diplomas and Bachelors Degrees); we also see the
importance of median family income and average family income for those
students considered independents (both income values in real 2015
dollars). Unexpected, however, were the appearances of 3-year cohort
default rates and the percentage of students who completed their
coursework within 8 years at the original institution (Satisfactory
Academic Progress).

**BS: I appreciate this. I'll probably make a general rewrite when I
wrap my head around the results better. But I'll most definitely use a
lot of this text.**

## Study Significance

While the technical nature of machine learning approaches can seem far
removed from the higher education policy landscape at large, this
study, at its foundation, cares about the material outcomes for
students investing their money and time in their educational
futures. More specifically, we are preoccupied with identifying the
greatest predictors of college graduates' incomes to ultimately inform
good policy & practice that amplifies positive student earnings
potential.

Machine learning approaches hold incredible posibilities in providing
the most accurate estimates of the data points we as higher education
practitioners/researchers care so deeply about: student outcomes. It
is evident that the integration of machine learning into higher
education research methods/practice has already begun, and this
project adds to this movement and solidifies its place in the higher
education policy landscape.

**BS: I think the CPPHE crowd will also want some more about the
policy. ML is cool, but what are getting for all of this? is what
they'll ask. This just means that we need to hit home that our
principled approach isn't just a cool technical exercise, but rather
will get us better estimates of student earnings (super important!)
than would otherwise get.**

\newpage

```{r}
cb_df<-read_rds(here("data","cleaned","cb_df.Rds"))
```

# Figures

## Figure 1 

### First Year Earnings of Bachelor's Degree Holders

```{r}
cb_df%>%
  group_by(cipdesc,creddesc)%>%
  select(earn_mdn_hi_1yr,creddesc,cipdesc)%>%
  mutate(count=n())%>%
  filter(count>20)%>%
  mutate(mean_earn=mean(earn_mdn_hi_1yr,na.rm=TRUE))%>%
  group_by(creddesc)%>%
  arrange(creddesc,desc(mean_earn)) %>% 
  mutate(earn_rank=rank(-mean_earn))%>%
  filter(earn_rank<2000)%>%
  filter(creddesc=="Bachelors Degree")%>%
  ggplot(aes(x=earn_mdn_hi_1yr,
             y=fct_reorder(cipdesc,.x=mean_earn),fill=cipdesc))+
  geom_density_ridges()+
  theme_minimal()+
  theme(legend.position = "none")+
  ylab("")+
  xlab("")+
  scale_x_continuous(labels=dollar_format())
```

\newpage

## Figure 2

### First Year Earnings of Associate Degree Holders
```{r}
cb_df%>%
  group_by(cipdesc,creddesc)%>%
  select(earn_mdn_hi_1yr,creddesc,cipdesc)%>%
  mutate(count=n())%>%
  filter(count>20)%>%
  mutate(mean_earn=mean(earn_mdn_hi_1yr,na.rm=TRUE))%>%
  group_by(creddesc)%>%
  arrange(creddesc,desc(mean_earn)) %>% 
  mutate(earn_rank=rank(-mean_earn))%>%
  filter(earn_rank<2000)%>%
  filter(creddesc=="Associate's Degree")%>%
  ggplot(aes(x=earn_mdn_hi_1yr,
             y=fct_reorder(cipdesc,.x=mean_earn),fill=cipdesc))+
  geom_density_ridges()+
  theme_minimal()+
  theme(legend.position = "none")+
  ylab("")+
  xlab("")+
  scale_x_continuous(labels=dollar_format())
```
\newpage

## Figure 3

### First Year Earnings of Certificate Holders
```{r}
cb_df%>%
  group_by(cipdesc,creddesc)%>%
  select(earn_mdn_hi_1yr,creddesc,cipdesc)%>%
  mutate(count=n())%>%
  filter(count>20)%>%
  mutate(mean_earn=mean(earn_mdn_hi_1yr,na.rm=TRUE))%>%
  group_by(creddesc)%>%
  arrange(creddesc,desc(mean_earn)) %>% 
  mutate(earn_rank=rank(-mean_earn))%>%
  filter(earn_rank<2000)%>%
  filter(creddesc=="Undergraduate Certificate or Diploma")%>%
  ggplot(aes(x=earn_mdn_hi_1yr,
             y=fct_reorder(cipdesc,.x=mean_earn),fill=cipdesc))+
  geom_density_ridges()+
  theme_minimal()+
  theme(legend.position = "none")+
  ylab("")+
  xlab("")+
  scale_x_continuous(labels=dollar_format())
```

\newpage

## Figure 4 

### Elastic Net Estimates


```{r}

load("../scripts/enet_vi.Rdata")

gg
```



\newpage


## Figure 5

### Random Forest Regression: Variable Importance

![](../figures/rf_vi.png)
